{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import HumanMessage\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo\")\n",
    "model_path = \"./sentence-camembert-large\"\n",
    "\n",
    "# Model and encoding configurations\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def split_paragraphs(rawText):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    return text_splitter.split_text(rawText)\n",
    "\n",
    "# Function to load PDFs and split text into chunks\n",
    "def load_txt_file(txt_path):\n",
    "    text_chunks = []    \n",
    "    loader = TextLoader(txt_path)\n",
    "    documents = loader.load()\n",
    "    for doc in documents:\n",
    "        chunks = split_paragraphs(doc.page_content)\n",
    "        text_chunks.extend(chunks)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = \"base_competence.txt\"\n",
    "base_chunks = load_txt_file(txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FAISS.from_texts(base_chunks, embedding_model)\n",
    "store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "query = \"INRAE\"\n",
    "k = 4\n",
    "docs = db.similarity_search_with_score(query, fetch_k=k)\n",
    "sorted_docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "for i in range(k):\n",
    "    print(sorted_docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt(retriever, query):\n",
    "    results = retriever.similarity_search(query, k=4)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\"Tu es un assistant et tu dois repondre aux questions qui te sont posées par un recruteur sur moi.\n",
    "Tu dois être consis et convainquand dans tes reponses. \n",
    "Voici le contexte dont tu dois te servir : \n",
    "\\nContexte :\\n{source_knowledge}\n",
    "\\nQuestion : \\n{query}\"\"\"\n",
    "    return augment_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"cites mois les langages que HTI utilise\"\n",
    "prompt = [\n",
    "    HumanMessage(content=custom_prompt(db, query))]\n",
    "res = llm.invoke(prompt)\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
