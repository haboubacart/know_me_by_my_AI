{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import HumanMessage\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo\")\n",
    "model_path = \"./sentence-camembert-large\"\n",
    "\n",
    "# Model and encoding configurations\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def split_paragraphs(rawText):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    return text_splitter.split_text(rawText)\n",
    "\n",
    "# Function to load PDFs and split text into chunks\n",
    "def load_txt_file(txt_path):\n",
    "    text_chunks = []    \n",
    "    loader = TextLoader(txt_path)\n",
    "    documents = loader.load()\n",
    "    for doc in documents:\n",
    "        chunks = split_paragraphs(doc.page_content)\n",
    "        text_chunks.extend(chunks)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 447, which is longer than the specified 200\n",
      "Created a chunk of size 332, which is longer than the specified 200\n",
      "Created a chunk of size 570, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 241, which is longer than the specified 200\n",
      "Created a chunk of size 501, which is longer than the specified 200\n",
      "Created a chunk of size 752, which is longer than the specified 200\n",
      "Created a chunk of size 384, which is longer than the specified 200\n",
      "Created a chunk of size 1648, which is longer than the specified 200\n",
      "Created a chunk of size 281, which is longer than the specified 200\n",
      "Created a chunk of size 509, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 1310, which is longer than the specified 200\n",
      "Created a chunk of size 436, which is longer than the specified 200\n",
      "Created a chunk of size 655, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "txt_path = \"base_competence.txt\"\n",
    "base_chunks = load_txt_file(txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FAISS.from_texts(base_chunks, embedding_model)\n",
    "store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(page_content=\"Compétences\\nTECHNIQUES : NLP, RAG, LLM,  analyse de données, reporting,, deep learning, machine learning, classification d'images, plan de test\\nDOMAINES FONCTIONNELS : système d'information, transport, science et recherche, formation professionnelle\\nMÉTHODOLOGIE DE GESTION DE PROJET : Agile Scrum\\nOUTILS : Power BI, Alteryx, Elasticsearch, Git, Microsoft Azure DevOps, Postman, SoapUI\\nLANGAGES DE DÉVELOPPEMENT : Python, SQL, Node.js\\nBASES DE DONNÉES : MySQL, PostgreSQL, Redis\\nPLATEFORMES : Google Cloud Platform, Microsoft Azure Databricks\\nLANGUES : Anglais (Courant)\"), 122.55694)\n",
      "(Document(page_content='Caisse des dépôts et de Consignation • Data Engineer • depuis 03/2024 (3 mois)\\nClub Data mc2i • Data Analyst • depuis 10/2022 (1 an et 8 mois)\\nIle-De-France Mobilités • Analytics Engineer • 11/2022 à 03/2024 (1 an et 5 mois)\\nPwC France • Data Scientist • 09/2021 à 08/2022 (1 an)\\nINRAE • Data Scientist • 04/2021 à 08/2021 (5 mois)'), 116.35309)\n",
      "(Document(page_content='Compétences pratiquées : API REST, data science, Microsoft Azure Databricks, Microsoft Power BI, Scikit-learn\\n\\n\\nINRAE • Data Scientist • 04/2021 à 08/2021 (5 mois) • Montpellier'), 105.026306)\n",
      "(Document(page_content=\"Au sein de l'UMR (Unité Mixte de Recherche) AGAP (Amélioration Génétique des Plantes) de l'INRAE (Institut National de Recherche pour l'Agriculture l'alimentation et l'Environnement) HTI est intervenu au sein du projet de développement d’une application permettant aux agriculteurs de détecter plus facilement des colmatages dans leur systèmes d'irrigation, à travers une classification des images des arbres du verger prises par drone.\"), 94.3416)\n"
     ]
    }
   ],
   "source": [
    "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "query = \"INRAE\"\n",
    "k = 4\n",
    "docs = db.similarity_search_with_score(query, fetch_k=k)\n",
    "sorted_docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "for i in range(k):\n",
    "    print(sorted_docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt(retriever, query):\n",
    "    results = retriever.similarity_search(query, k=3)\n",
    "    source_knowledge = \" \".join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\"Tu es un assistant et tu dois repondre aux questions qui te sont posées, en utilisant le contexte suivant: \\n\\nContexte :\\n{source_knowledge}\\n\\nQuestion : \\n{query}\"\"\"\n",
    "    return augment_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTI utilise principalement les langages de programmation tels que Python, SQL et R dans le cadre de son travail en data analytics, genAI et l'automatisation des tests fonctionnels. Il a également une bonne maîtrise en traitement de langage naturel (NLP) pour la mise en place de chatbots basés sur la technologie RAG et LLM.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"cites mois les langages que HTI utilise\"\n",
    "prompt = [\n",
    "    HumanMessage(content=custom_prompt(db, query))]\n",
    "res = llm.invoke(prompt)\n",
    "print(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
