{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Lajavaness/sentence-camembert-large\"\n",
    "\n",
    "# Model and encoding configurations\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def split_paragraphs(text, type_text):\n",
    "    splitter = {\n",
    "            \"txt\" : CharacterTextSplitter,\n",
    "            \"md\" : MarkdownTextSplitter\n",
    "    }\n",
    "    text_splitter = splitter[type_text](\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Function to load PDFs and split text into chunks\n",
    "def load_file(file_path, file_type):\n",
    "    Loader = {\n",
    "            \"txt\" : TextLoader,\n",
    "            \"md\" : UnstructuredMarkdownLoader\n",
    "    }\n",
    "    text_chunks = []    \n",
    "    loader = Loader[file_type](file_path)\n",
    "    documents = loader.load()\n",
    "    for doc in documents:\n",
    "        chunks = split_paragraphs(doc.page_content, file_type)\n",
    "        text_chunks.extend(chunks)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''txt_path = \"base_competence.txt\"\n",
    "file_type = \"txt\"'''\n",
    "\n",
    "txt_path = \"base_competence.md\"\n",
    "file_type = \"md\"\n",
    "base_chunks = load_file(txt_path, file_type)\n",
    "store = FAISS.from_texts(base_chunks, embedding_model)\n",
    "store.save_local(\"chatbot/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\"chatbot/faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "query = \"quels sont les comp√©tences de ...\"\n",
    "docs = db.similarity_search_with_score(query, fetch_k=4)\n",
    "for i in range(len(docs)):\n",
    "    print(docs[i][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
